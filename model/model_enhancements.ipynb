{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29f3c01",
   "metadata": {},
   "source": [
    "# Malnutrition Model Enhancements\n",
    "\n",
    "This notebook implements critical improvements to make the model production-ready:\n",
    "\n",
    "1. WHO Standards Integration\n",
    "2. Enhanced Feature Engineering\n",
    "3. Confidence Scoring\n",
    "4. Comprehensive Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e125e",
   "metadata": {},
   "source": [
    "## 1. WHO Standards Integration\n",
    "\n",
    "Adding WHO z-scores calculation for:\n",
    "- Weight-for-age\n",
    "- Height-for-age\n",
    "- BMI-for-age\n",
    "- MUAC-for-age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91049f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zscore(measurement, age_months, gender, indicator):\n",
    "    \"\"\"Calculate z-score based on WHO standards\n",
    "    \n",
    "    Args:\n",
    "        measurement (float): The measured value (weight in kg, height in cm, etc)\n",
    "        age_months (float): Age in months\n",
    "        gender (str): 'M' or 'F'\n",
    "        indicator (str): One of 'wfa' (weight-for-age), 'hfa' (height-for-age),\n",
    "                        'bfa' (bmi-for-age), 'mfa' (muac-for-age)\n",
    "    \"\"\"\n",
    "    # TODO: Implement WHO standard calculations\n",
    "    # For now using a simplified calculation based on normal distribution\n",
    "    # This should be replaced with actual WHO LMS values\n",
    "    \n",
    "    if indicator == 'wfa':\n",
    "        # Simplified weight-for-age z-score\n",
    "        expected_weight = (age_months * 0.3) + 3.5  # Very simplified\n",
    "        sd = expected_weight * 0.1\n",
    "        return (measurement - expected_weight) / sd\n",
    "    \n",
    "    elif indicator == 'hfa':\n",
    "        # Simplified height-for-age z-score\n",
    "        expected_height = (age_months * 0.5) + 50  # Very simplified\n",
    "        sd = expected_height * 0.05\n",
    "        return (measurement - expected_height) / sd\n",
    "    \n",
    "    elif indicator == 'bfa':\n",
    "        # Simplified BMI-for-age z-score\n",
    "        expected_bmi = 16 + (age_months * 0.05)  # Very simplified\n",
    "        sd = 2\n",
    "        return (measurement - expected_bmi) / sd\n",
    "    \n",
    "    elif indicator == 'mfa':\n",
    "        # Simplified MUAC-for-age z-score\n",
    "        expected_muac = 14 + (age_months * 0.02)  # Very simplified\n",
    "        sd = 1.5\n",
    "        return (measurement - expected_muac) / sd\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf842a6",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_enhanced_features(df):\n",
    "    \"\"\"Add enhanced features including WHO z-scores and clinical indicators\"\"\"\n",
    "    \n",
    "    enhanced = df.copy()\n",
    "    \n",
    "    # Calculate WHO z-scores\n",
    "    enhanced['weight_for_age_z'] = enhanced.apply(\n",
    "        lambda x: calculate_zscore(x['weight_kg'], x['age_months'], x.get('gender', 'F'), 'wfa'), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enhanced['height_for_age_z'] = enhanced.apply(\n",
    "        lambda x: calculate_zscore(x['height_cm'], x['age_months'], x.get('gender', 'F'), 'hfa'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enhanced['bmi_for_age_z'] = enhanced.apply(\n",
    "        lambda x: calculate_zscore(x['bmi'], x['age_months'], x.get('gender', 'F'), 'bfa'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enhanced['muac_for_age_z'] = enhanced.apply(\n",
    "        lambda x: calculate_zscore(x['muac_cm'], x['age_months'], x.get('gender', 'F'), 'mfa'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add composite indicators\n",
    "    enhanced['stunting_score'] = enhanced['height_for_age_z'].apply(lambda x: 1 if x < -2 else 0)\n",
    "    enhanced['wasting_score'] = enhanced['weight_for_age_z'].apply(lambda x: 1 if x < -2 else 0)\n",
    "    enhanced['underweight_score'] = enhanced['bmi_for_age_z'].apply(lambda x: 1 if x < -2 else 0)\n",
    "    \n",
    "    # Add interaction terms\n",
    "    enhanced['weight_height_interaction'] = enhanced['weight_for_age_z'] * enhanced['height_for_age_z']\n",
    "    enhanced['bmi_muac_interaction'] = enhanced['bmi_for_age_z'] * enhanced['muac_for_age_z']\n",
    "    \n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403bb9d",
   "metadata": {},
   "source": [
    "## 3. Confidence Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidentMalnutritionClassifier:\n",
    "    \"\"\"Wrapper class that adds confidence scoring to predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        self.model = base_model\n",
    "        self.confidence_threshold = 0.8  # Minimum confidence for reliable predictions\n",
    "    \n",
    "    def calculate_confidence(self, features, prediction_proba):\n",
    "        \"\"\"Calculate confidence score based on:\n",
    "        1. Model's prediction probability\n",
    "        2. Feature reliability\n",
    "        3. Distance from decision boundary\n",
    "        \"\"\"\n",
    "        # Base confidence from model probability\n",
    "        max_prob = np.max(prediction_proba)\n",
    "        \n",
    "        # Feature reliability score (check for outliers)\n",
    "        feature_reliability = self._check_feature_reliability(features)\n",
    "        \n",
    "        # Combine scores\n",
    "        confidence = (max_prob * 0.7) + (feature_reliability * 0.3)\n",
    "        \n",
    "        return confidence\n",
    "    \n",
    "    def _check_feature_reliability(self, features):\n",
    "        \"\"\"Check if features are within expected ranges\"\"\"\n",
    "        reliability_scores = []\n",
    "        \n",
    "        # Check z-scores are within reasonable ranges\n",
    "        for col in features.columns:\n",
    "            if 'z' in col:\n",
    "                z_scores = features[col]\n",
    "                # Give lower reliability for extreme z-scores\n",
    "                reliability = 1 - (np.abs(z_scores) > 3).mean()\n",
    "                reliability_scores.append(reliability)\n",
    "        \n",
    "        return np.mean(reliability_scores) if reliability_scores else 0.5\n",
    "    \n",
    "    def predict_with_confidence(self, X):\n",
    "        \"\"\"Make predictions with confidence scores\"\"\"\n",
    "        predictions = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        confidences = [self.calculate_confidence(X.iloc[i:i+1], prob) \n",
    "                      for i, prob in enumerate(probabilities)]\n",
    "        \n",
    "        return predictions, confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db47fd",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_thoroughly(model, X, y):\n",
    "    \"\"\"Perform comprehensive model validation\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Stratified K-Fold Cross Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=skf)\n",
    "    results['cv_scores'] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    # 2. Hold-out Test Set Validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 3. Calculate various metrics\n",
    "    results['classification_report'] = classification_report(y_test, y_pred, output_dict=True)\n",
    "    results['confusion_matrix'] = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # 4. Check for bias in predictions\n",
    "    results['bias_check'] = check_prediction_bias(model, X_test, y_test)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_prediction_bias(model, X, y_true):\n",
    "    \"\"\"Check for systematic bias in predictions\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Check for bias in different value ranges\n",
    "    bias_results = {}\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in [np.float64, np.int64]:\n",
    "            # Split into quartiles\n",
    "            quartiles = pd.qcut(X[col], q=4)\n",
    "            \n",
    "            # Calculate accuracy for each quartile\n",
    "            accuracies = []\n",
    "            for q in quartiles.unique():\n",
    "                mask = quartiles == q\n",
    "                acc = (y_true[mask] == y_pred[mask]).mean()\n",
    "                accuracies.append(acc)\n",
    "            \n",
    "            # Check if accuracies are significantly different\n",
    "            f_stat, p_value = stats.f_oneway(*[[acc] for acc in accuracies])\n",
    "            \n",
    "            bias_results[col] = {\n",
    "                'quartile_accuracies': accuracies,\n",
    "                'p_value': p_value,\n",
    "                'has_bias': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    return bias_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396feb",
   "metadata": {},
   "source": [
    "# Model Monitoring System\n",
    "\n",
    "We'll implement a monitoring system to track:\n",
    "1. Model performance over time\n",
    "2. Data drift detection\n",
    "3. Prediction quality metrics\n",
    "4. System health indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92935035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor:\n",
    "    \"\"\"System for monitoring model performance and data quality\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, feature_names):\n",
    "        self.model_name = model_name\n",
    "        self.feature_names = feature_names\n",
    "        self.baseline_stats = None\n",
    "        self.performance_log = []\n",
    "        \n",
    "    def set_baseline_statistics(self, X_baseline):\n",
    "        \"\"\"Calculate and store baseline statistics for drift detection\"\"\"\n",
    "        self.baseline_stats = {\n",
    "            'feature_means': X_baseline.mean().to_dict(),\n",
    "            'feature_stds': X_baseline.std().to_dict(),\n",
    "            'feature_ranges': {\n",
    "                col: (X_baseline[col].min(), X_baseline[col].max())\n",
    "                for col in X_baseline.columns\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def check_data_drift(self, new_data):\n",
    "        \"\"\"Check for significant drift in feature distributions\"\"\"\n",
    "        if self.baseline_stats is None:\n",
    "            raise ValueError(\"Baseline statistics not set. Call set_baseline_statistics first.\")\n",
    "        \n",
    "        drift_report = {\n",
    "            'feature_drift': {},\n",
    "            'severe_drift_detected': False\n",
    "        }\n",
    "        \n",
    "        for feature in self.feature_names:\n",
    "            if feature not in new_data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Calculate drift metrics\n",
    "            mean_drift = abs(new_data[feature].mean() - self.baseline_stats['feature_means'][feature])\n",
    "            std_drift = abs(new_data[feature].std() - self.baseline_stats['feature_stds'][feature])\n",
    "            \n",
    "            # Normalize drift by baseline std\n",
    "            normalized_drift = mean_drift / self.baseline_stats['feature_stds'][feature]\n",
    "            \n",
    "            drift_report['feature_drift'][feature] = {\n",
    "                'mean_drift': mean_drift,\n",
    "                'std_drift': std_drift,\n",
    "                'normalized_drift': normalized_drift,\n",
    "                'is_severe': normalized_drift > 0.5  # Threshold for severe drift\n",
    "            }\n",
    "            \n",
    "            if normalized_drift > 0.5:\n",
    "                drift_report['severe_drift_detected'] = True\n",
    "        \n",
    "        return drift_report\n",
    "    \n",
    "    def log_prediction_quality(self, y_true, y_pred, confidences):\n",
    "        \"\"\"Log prediction quality metrics\"\"\"\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "        \n",
    "        metrics = {\n",
    "            'timestamp': pd.Timestamp.now(),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'mean_confidence': np.mean(confidences),\n",
    "            'low_confidence_rate': (np.array(confidences) < 0.8).mean()\n",
    "        }\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "        metrics.update({\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "        \n",
    "        self.performance_log.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def generate_monitoring_report(self):\n",
    "        \"\"\"Generate comprehensive monitoring report\"\"\"\n",
    "        if not self.performance_log:\n",
    "            return \"No performance data available yet.\"\n",
    "        \n",
    "        df_metrics = pd.DataFrame(self.performance_log)\n",
    "        \n",
    "        report = {\n",
    "            'latest_metrics': df_metrics.iloc[-1].to_dict(),\n",
    "            'trend_analysis': {\n",
    "                'accuracy_trend': df_metrics['accuracy'].rolling(5).mean().iloc[-1],\n",
    "                'confidence_trend': df_metrics['mean_confidence'].rolling(5).mean().iloc[-1]\n",
    "            },\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        # Check for concerning patterns\n",
    "        if df_metrics['accuracy'].iloc[-1] < df_metrics['accuracy'].mean() - df_metrics['accuracy'].std():\n",
    "            report['alerts'].append(\"Recent accuracy drop detected\")\n",
    "            \n",
    "        if df_metrics['low_confidence_rate'].iloc[-1] > 0.2:\n",
    "            report['alerts'].append(\"High rate of low confidence predictions\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with existing model\n",
    "print(\"Loading existing model and data...\")\n",
    "\n",
    "# Load the data\n",
    "malnutrition_data = pd.read_csv('preprocessed_malnutrition_data.csv')\n",
    "model = joblib.load('malnutrition_model.joblib')\n",
    "scaler = joblib.load('feature_scaler.joblib')\n",
    "\n",
    "# Create enhanced features\n",
    "print(\"\\nApplying enhanced feature engineering...\")\n",
    "enhanced_data = engineer_enhanced_features(malnutrition_data)\n",
    "\n",
    "# Initialize confident classifier and model monitor\n",
    "confident_classifier = ConfidentMalnutritionClassifier(model)\n",
    "monitor = ModelMonitor('malnutrition_model_v2', enhanced_data.columns)\n",
    "\n",
    "# Set baseline statistics\n",
    "print(\"\\nSetting up monitoring baseline...\")\n",
    "monitor.set_baseline_statistics(enhanced_data)\n",
    "\n",
    "# Prepare features and target\n",
    "X = enhanced_data.drop(['nutrition_status'], axis=1)\n",
    "y = malnutrition_data['nutrition_status']\n",
    "\n",
    "# Validate model with new features\n",
    "print(\"\\nPerforming comprehensive validation...\")\n",
    "validation_results = validate_model_thoroughly(model, X, y)\n",
    "\n",
    "# Make predictions with confidence scores\n",
    "predictions, confidences = confident_classifier.predict_with_confidence(X)\n",
    "\n",
    "# Log initial performance\n",
    "print(\"\\nLogging initial performance metrics...\")\n",
    "monitoring_metrics = monitor.log_prediction_quality(y, predictions, confidences)\n",
    "\n",
    "print(\"\\nInitial Monitoring Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy: {monitoring_metrics['accuracy']:.4f}\")\n",
    "print(f\"Mean Confidence: {monitoring_metrics['mean_confidence']:.4f}\")\n",
    "print(f\"Low Confidence Rate: {monitoring_metrics['low_confidence_rate']:.4f}\")\n",
    "\n",
    "if monitoring_metrics['low_confidence_rate'] > 0.2:\n",
    "    print(\"\\nWARNING: High rate of low confidence predictions detected!\")\n",
    "    \n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Cross-validation Score: {validation_results['cv_scores']['mean']:.4f} (+/- {validation_results['cv_scores']['std']*2:.4f})\")\n",
    "\n",
    "# Check for data drift\n",
    "drift_report = monitor.check_data_drift(X)\n",
    "if drift_report['severe_drift_detected']:\n",
    "    print(\"\\nWARNING: Severe data drift detected in some features!\")\n",
    "    for feature, stats in drift_report['feature_drift'].items():\n",
    "        if stats['is_severe']:\n",
    "            print(f\"- {feature}: Normalized drift = {stats['normalized_drift']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e740e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_if_needed(model, X, y, monitor, model_path='malnutrition_model.joblib', scaler_path='feature_scaler.joblib', min_accuracy_improvement=0.01, low_confidence_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Retrain model when monitoring signals indicate it's necessary.\n",
    "\n",
    "    Strategy:\n",
    "    - Check data drift via the monitor\n",
    "    - Check recent low-confidence rate from monitor logs\n",
    "    - Retrain on a fresh split and evaluate on holdout\n",
    "    - If severe drift OR low-confidence OR new model improves accuracy by min_accuracy_improvement,\n",
    "      accept the retrained model, save versioned artifacts and update baseline statistics.\n",
    "\n",
    "    Returns: (model, scaler, retrained_bool, info_dict)\n",
    "    \"\"\"\n",
    "    import os, joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "    # Safety checks\n",
    "    if X.shape[0] < 50:\n",
    "        return model, joblib.load(scaler_path) if os.path.exists(scaler_path) else None, False, {'reason': 'not_enough_data'}\n",
    "\n",
    "    drift = monitor.check_data_drift(X)\n",
    "    last_metrics = monitor.performance_log[-1] if monitor.performance_log else None\n",
    "    low_confidence = (last_metrics and last_metrics.get('low_confidence_rate', 0) > low_confidence_threshold)\n",
    "    severe_drift = drift.get('severe_drift_detected', False)\n",
    "\n",
    "    # Prepare a retrain dataset\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "    # Train a fresh model with the same hyperparameters used elsewhere\n",
    "    new_model = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    new_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    new_acc = new_model.score(X_holdout_scaled, y_holdout)\n",
    "    old_acc = last_metrics['accuracy'] if last_metrics and 'accuracy' in last_metrics else 0\n",
    "\n",
    "    retrain = False\n",
    "    reasons = []\n",
    "    if severe_drift:\n",
    "        retrain = True\n",
    "        reasons.append('severe_drift')\n",
    "    if low_confidence:\n",
    "        retrain = True\n",
    "        reasons.append('low_confidence')\n",
    "    if new_acc > old_acc + min_accuracy_improvement:\n",
    "        retrain = True\n",
    "        reasons.append('accuracy_improvement')\n",
    "\n",
    "    info = {\n",
    "        'timestamp': str(pd.Timestamp.now()),\n",
    "        'old_accuracy': old_acc,\n",
    "        'new_accuracy': float(new_acc),\n",
    "        'retrain': bool(retrain),\n",
    "        'reasons': reasons,\n",
    "        'drift_summary': drift\n",
    "    }\n",
    "\n",
    "    if retrain:\n",
    "        # Save versioned artifacts\n",
    "        ts = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        versioned_model = f\"malnutrition_model_{ts}.joblib\"\n",
    "        versioned_scaler = f\"feature_scaler_{ts}.joblib\"\n",
    "\n",
    "        joblib.dump(new_model, versioned_model)\n",
    "        joblib.dump(scaler, versioned_scaler)\n",
    "\n",
    "        # Also write canonical names used by the rest of the system\n",
    "        joblib.dump(new_model, model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "        # Update monitoring baseline to match the new data distribution\n",
    "        try:\n",
    "            monitor.set_baseline_statistics(X)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Append retrain event to log\n",
    "        log_row = info.copy()\n",
    "        log_row.update({\n",
    "            'model_file': versioned_model,\n",
    "            'scaler_file': versioned_scaler\n",
    "        })\n",
    "\n",
    "        import csv\n",
    "        log_file = 'retrain_log.csv'\n",
    "        write_header = not os.path.exists(log_file)\n",
    "        with open(log_file, 'a', newline='', encoding='utf-8') as fh:\n",
    "            writer = csv.DictWriter(fh, fieldnames=list(log_row.keys()))\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(log_row)\n",
    "\n",
    "        return new_model, scaler, True, info\n",
    "\n",
    "    # No retrain accepted: return original model and existing scaler (if available)\n",
    "    existing_scaler = None\n",
    "    try:\n",
    "        existing_scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "    except Exception:\n",
    "        existing_scaler = None\n",
    "\n",
    "    return model, existing_scaler, False, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec418cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call retraining trigger after drift check\n",
    "print('\\nChecking whether automated retraining is needed...')\n",
    "new_model, new_scaler, retrained, retrain_info = retrain_if_needed(model, X, y, monitor)\n",
    "if retrained:\n",
    "    model = new_model\n",
    "    scaler = new_scaler\n",
    "    print('\\nModel was retrained and artifacts updated:')\n",
    "    print(retrain_info)\n",
    "else:\n",
    "    print('\\nNo retraining performed. Info:')\n",
    "    print(retrain_info)\n",
    "\n",
    "# Mark todo 1 as completed in the local todo list\n",
    "# (The remote todo tool is authoritative; we'll update the manager after tests.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
